{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a508883-66a4-4304-b7ed-dd5431398fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb3b18a-94bc-4a66-bf31-dbf95fdccc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'activation Sigmoïde\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Fonction d'activation ReLU\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Fonction de dérivée pour Sigmoïde et ReLU\n",
    "def derivee_sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def derivee_relu(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93ee448-3927-453b-95a1-1418bc89cacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagation avant à travers le réseau\n",
    "def propagation_avant(entree, poids, biais):\n",
    "    activation = entree\n",
    "    activations = [activation]  # stocke les activations de chaque couche pour le backprop\n",
    "\n",
    "    # Propagation à travers chaque couche cachée\n",
    "    for i in range(len(poids) - 1):\n",
    "        z = np.dot(activation, poids[i]) + biais[i]\n",
    "        activation = relu(z)  # on utilise ReLU pour les couches cachées\n",
    "        activations.append(activation)\n",
    "    \n",
    "    # Couche de sortie (par ex., sigmoid pour une tâche de classification binaire)\n",
    "    z = np.dot(activation, poids[-1]) + biais[-1]\n",
    "    activation = sigmoid(z)\n",
    "    activations.append(activation)\n",
    "\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c93a4c-a737-4bf0-9838-78bd6a0e1acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de perte (Binary Cross-Entropy)\n",
    "def calcul_perte(y_pred, y_vrai):\n",
    "    m = y_vrai.shape[0]\n",
    "    perte = -np.sum(y_vrai * np.log(y_pred) + (1 - y_vrai) * np.log(1 - y_pred)) / m\n",
    "    return perte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb04a96-8f57-4803-b6b1-6742e16c5cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rétropropagation\n",
    "def retropropagation(activations, poids, biais, y_vrai, taux_apprentissage=0.01):\n",
    "    # Étape 1 : Calculer le gradient de la perte pour la couche de sortie\n",
    "    erreur = activations[-1] - y_vrai\n",
    "    deltas = [erreur * derivee_sigmoid(activations[-1])]\n",
    "\n",
    "    # Étape 2 : Calculer les gradients pour chaque couche cachée\n",
    "    for i in reversed(range(len(poids) - 1)):\n",
    "        delta = np.dot(deltas[-1], poids[i + 1].T) * derivee_relu(activations[i + 1])\n",
    "        deltas.append(delta)\n",
    "\n",
    "    # Inverser les deltas pour qu'ils correspondent à chaque couche du réseau\n",
    "    deltas = deltas[::-1]\n",
    "\n",
    "    # Mise à jour des poids et biais\n",
    "    for i in range(len(poids)):\n",
    "        poids[i] -= taux_apprentissage * np.dot(activations[i].T, deltas[i])\n",
    "        biais[i] -= taux_apprentissage * np.sum(deltas[i], axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3acb09-9c3a-4da0-b364-1ff19ac09d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'entraînement\n",
    "def entrainer_mlp(X, y, couches, epochs=1000, taux_apprentissage=0.01):\n",
    "    # Initialisation des poids et biais\n",
    "    poids = [np.random.rand(couches[i], couches[i + 1]) for i in range(len(couches) - 1)]\n",
    "    biais = [np.random.rand(1, couches[i + 1]) for i in range(len(couches) - 1)]\n",
    "    \n",
    "    # Boucle d'entraînement\n",
    "    for epoch in range(epochs):\n",
    "        # Propagation avant\n",
    "        activations = propagation_avant(X, poids, biais)\n",
    "        \n",
    "        # Calcul de la perte\n",
    "        perte = calcul_perte(activations[-1], y)\n",
    "        \n",
    "        # Rétropropagation\n",
    "        retropropagation(activations, poids, biais, y, taux_apprentissage)\n",
    "        \n",
    "        # Afficher la perte à intervalles réguliers\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Perte: {perte:.4f}\")\n",
    "    \n",
    "    return poids, biais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d171b4-33fd-4dbe-9426-ceb668fc0915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de prédiction\n",
    "def predire(X, poids, biais):\n",
    "    activations = propagation_avant(X, poids, biais)\n",
    "    return activations[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1d859-708e-4d05-9789-f58cbc3c861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 1 : Charger les données Iris\n",
    "iris = load_iris()\n",
    "X = iris.data  # caractéristiques\n",
    "y = iris.target  # étiquettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f2ca4-2699-4fdc-8127-6285c3c2c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 2 : Prétraitement des données\n",
    "# Normaliser les caractéristiques\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8075e9e-dadc-47e7-bdb7-fb2902b79f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding des étiquettes\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d981ecc3-380f-42b7-953b-50d8d52049f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 3 : Entraîner le MLP\n",
    "couches = [4, 5, 3]  # 4 neurones d'entrée, 5 neurones cachés, 3 neurones de sortie\n",
    "poids, biais = entrainer_mlp(X_train, y_train, couches, epochs=1000, taux_apprentissage=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79185769-52b8-4312-8877-00883ff76029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 4 : Faire des prédictions\n",
    "y_pred = predire(X_test, poids, biais)\n",
    "\n",
    "# Convertir les probabilités en classes prédites\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Convertir les classes réelles pour évaluer les performances\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Évaluer les résultats\n",
    "accuracy = np.mean(y_pred_classes == y_test_classes)\n",
    "print(f\"Accuracy sur l'ensemble de test : {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
