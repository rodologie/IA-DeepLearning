{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice 3.1 : Classification de Sentiment avec Augmentation du Nombre de Neurones\n",
        "**Objectif** : Créer un modèle simple de classification de texte pour classer les critiques de films en positives ou négatives. Commencez avec un petit nombre de neurones et augmentez progressivement ce nombre pour observer les effets sur la performance.\n",
        "\n",
        "**Données** : Utilisez un sous-ensemble du jeu de données de critiques de films IMDB.\n",
        "\n",
        "**Étapes** :\n",
        "  1. Commencez par une couche `Embedding`, suivie d'une couche `Dense` avec 16 neurones.\n",
        "  2. Entraînez le modèle et enregistrez la précision.\n",
        "  3. Augmentez progressivement le nombre de neurones dans la couche `Dense` (par exemple, de 16 à 64 puis à 128) et observez comment la précision et le temps d'entraînement sont affectés.\n",
        "  4. Tracez les résultats pour la précision et la perte, y compris pour les tests de validation.\n",
        "\n",
        "Complétez le code ci-dessous."
      ],
      "metadata": {
        "id": "8_5CliF5f9uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "PDauPHSTibC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "vocab_size = 10000  # Only consider the top 10k words\n",
        "max_length = 256    # Pad/truncate all reviews to be 256 words\n",
        "\n",
        "# Load IMDB data\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "# Pad sequences to the same length\n",
        "x_train = pad_sequences(x_train, maxlen=max_length)\n",
        "x_test = pad_sequences(x_test, maxlen=max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9Ee5DzRie6X",
        "outputId": "79e0b22d-b6f7-4d9d-c648-0260895988c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(dense_neurons):\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Embedding(input_dim=vocab_size, output_dim=64, input_length=max_length),\n",
        "        layers.GlobalAveragePooling1D(),\n",
        "        layers.Dense(dense_neurons, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "5wgXwCGvUsiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neuron_counts = [16, 64, 128]  # Different sizes for the Dense layer\n",
        "results = {}\n",
        "\n",
        "for neurons in neuron_counts:\n",
        "    print(f\"\\nTraining model with {neurons} neurons in the Dense layer\")\n",
        "    model = build_model(dense_neurons=neurons)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        epochs=5,\n",
        "        batch_size=512,\n",
        "        validation_split=0.2,\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(x_test, y_test, verbose=2)\n",
        "    results[neurons] = {'accuracy': accuracy, 'loss': loss}\n"
      ],
      "metadata": {
        "id": "6ipeueguUun5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice 3.2 : Classification d'Images Fine-grained avec Augmentation du Nombre de Neurones\n",
        "**Objectif** : Créer un modèle de classification d'images de base pour classer des images de fleurs (`oxford_flowers102` - le *Oxford 102 Flower Dataset*). Commencez avec un petit nombre de neurones et augmentez-les progressivement pour observer les effets sur la performance.\n",
        "\n",
        "**Données** : Utilisez le *Oxford 102 Flower Dataset*, qui contient des images de 102 catégories de fleurs.\n",
        "\n",
        "**Étapes** :\n",
        "  1. Commencez avec des couches `Conv2D` et `MaxPooling2D` pour l'extraction des caractéristiques des images.\n",
        "  2. Ajoutez une couche `Flatten` pour convertir les cartes de caractéristiques 2D en un vecteur 1D.\n",
        "  3. Ajoutez une couche `Dense` avec un petit nombre de neurones (par exemple, 32), suivie d'une couche de sortie.\n",
        "  4. Entraînez le modèle et enregistrez la précision.\n",
        "  5. Augmentez progressivement le nombre de neurones dans la couche `Dense` (par exemple, de 32 à 128 puis à 256) pour observer les changements dans la précision et le temps d'entraînement.\n",
        "\n",
        "Complétez le code ci-dessous."
      ],
      "metadata": {
        "id": "I3VGLIKtiVxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "UR5TdXBPixMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Oxford 102 Flower Dataset\n",
        "dataset, info = tfds.load('oxford_flowers102', with_info=True, as_supervised=True)\n",
        "\n",
        "# Split the dataset into training and testing\n",
        "train_dataset = dataset['train']\n",
        "test_dataset = dataset['test']"
      ],
      "metadata": {
        "id": "UV9kuvwDizpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set image parameters\n",
        "image_size = (150, 150)  # Resize images to this size\n",
        "batch_size = 32\n",
        "\n",
        "# Data preprocessing function to resize images and normalize pixel values\n",
        "def preprocess_image(image, label):\n",
        "    image = tf.image.resize(image, image_size)\n",
        "    image = image / 255.0  # Normalize pixel values to [0, 1]\n",
        "    return image, label\n",
        "\n",
        "# Apply preprocessing to the train and test datasets\n",
        "train_dataset = train_dataset.map(preprocess_image).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = test_dataset.map(preprocess_image).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "3oUuihUTi4kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice 3.3 : **Étudier l'Influence de la Taille des Lots sur la Performance du Modèle**\n",
        "\n",
        "**Objectif** : Cet exercice démontre comment différentes tailles de lots affectent la performance d'un modèle de réseau neuronal, y compris sa vitesse d'entraînement, sa perte, sa précision et sa capacité de généralisation.\n",
        "\n",
        "**Jeu de données** : **Fashion MNIST** — un jeu de données contenant des images en niveaux de gris de 10 types de vêtements différents, avec 60 000 images d'entraînement et 10 000 images de test.\n",
        "\n",
        "---\n",
        "\n",
        "### Étapes :\n",
        "\n",
        "#### 1. **Charger le Jeu de Données**\n",
        "   - Utilisez `tensorflow.keras.datasets.fashion_mnist` pour charger le jeu de données.\n",
        "   - Prétraitez les données en normalisant les valeurs des pixels dans l'intervalle `[0, 1]`.\n",
        "\n",
        "#### 2. **Définir l'Architecture du Modèle**\n",
        "   - Construisez un modèle simple de réseau neuronal convolutionnel (CNN) ou de réseau neuronal complètement connecté (FCNN).\n",
        "   - Le modèle doit inclure :\n",
        "     - Une couche d'entrée (pour gérer les images 28x28).\n",
        "     - Une ou plusieurs couches cachées (par exemple, Dense, Conv2D).\n",
        "     - Une couche de sortie avec 10 unités (une pour chaque catégorie de vêtement).\n",
        "   - Utilisez **l'activation softmax** pour la couche de sortie, car il s'agit d'un problème de classification multiclasse.\n",
        "\n",
        "#### 3. **Varier la Taille des Lots**\n",
        "   - Expérimentez avec différentes tailles de lots (par exemple, 16, 32, 64, 128, 256).\n",
        "   - Pour chaque taille de lot :\n",
        "     - Entraînez le modèle pendant un nombre fixe d'époques (par exemple, 10 époques).\n",
        "     - Enregistrez la perte d'entraînement, la perte de validation et la précision.\n",
        "\n",
        "#### 4. **Entraîner le Modèle**\n",
        "   - Entraînez le modèle pour chaque taille de lot et mesurez les éléments suivants :\n",
        "     - **Temps d'entraînement** : Combien de temps il faut pour terminer une époque.\n",
        "     - **Perte d'entraînement et de validation** : Suivez l'évolution de la perte pendant l'entraînement.\n",
        "     - **Précision** : Suivez les performances du modèle sur les données d'entraînement et de validation.\n",
        "\n",
        "#### 5. **Analyser les Résultats**\n",
        "   - Comparez les éléments suivants :\n",
        "     - **Temps d'entraînement** : Les tailles de lots plus grandes peuvent conduire à un entraînement plus rapide, mais elles pourraient aussi entraîner des rendements décroissants en termes de performance du modèle.\n",
        "     - **Perte et précision** : Observez comment la taille du lot affecte la convergence de la fonction de perte et la précision sur les ensembles de données d'entraînement et de validation.\n",
        "     - **Surapprentissage** : Vérifiez si des tailles de lots plus petites mènent à une meilleure généralisation (perte de validation plus faible) ou si des tailles de lots plus grandes entraînent un surapprentissage.\n",
        "\n",
        "#### 6. **Tracer les Résultats**\n",
        "   - Tracez des graphiques comparant la perte d'entraînement, la perte de validation et la précision pour différentes tailles de lots.\n",
        "   - Tracez le temps d'entraînement pour différentes tailles de lots."
      ],
      "metadata": {
        "id": "yHWa7oi9VCTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import fashion_mnist"
      ],
      "metadata": {
        "id": "vofqzMkKVBS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Reshape the images to (28, 28, 1) to match the input shape expected by CNNs\n",
        "train_images = train_images.reshape(-1, 28, 28, 1)\n",
        "test_images = test_images.reshape(-1, 28, 28, 1)"
      ],
      "metadata": {
        "id": "fm_yDvjqVYPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercice 3.4 : Classification des Émotions avec le Jeu de Données CREMA-D\n",
        "\n",
        "**Objectif** : Créer un modèle pour classifier les émotions à partir de clips audio de discours humain. Cet exercice se concentre sur l'identification des émotions telles que la colère, la joie, la tristesse et les tons neutres, en utilisant un prétraitement audio de base et un réseau neuronal convolutionnel.\n",
        "\n",
        "**Jeu de données** : **CREMA-D** (Crowd-sourced Emotional Multimodal Actors Dataset) contient des clips audio d'acteurs exprimant six émotions : colère, dégoût, peur, joie, neutre et tristesse. Bien qu'il ne soit pas directement disponible dans `tensorflow_datasets`, il est suffisamment petit pour être prétraité et chargé efficacement dans TensorFlow.\n",
        "\n",
        "---\n",
        "\n",
        "### Étapes :\n",
        "\n",
        "#### 1. **Chargement et Prétraitement des Données**\n",
        "   - **Charger le Jeu de Données** :\n",
        "     - Téléchargez le jeu de données CREMA-D depuis sa [source officielle](https://github.com/CheyneyComputerScience/CREMA-D).\n",
        "     - Organisez les fichiers audio et les étiquettes d'émotions correspondantes.\n",
        "   - **Traitement Audio** :\n",
        "     - Convertissez les formes d'onde audio en spectrogrammes ou en mel-spectrogrammes pour chaque clip audio.\n",
        "     - Normalisez les spectrogrammes et ajustez-les pour qu'ils aient une longueur constante, par exemple, 2 secondes.\n",
        "\n",
        "#### 2. **Construire un Modèle Simple de Classification des Émotions**\n",
        "   - **Couches Convolutionnelles** :\n",
        "     - Commencez par une couche `Conv2D` pour apprendre les motifs spatiaux dans le spectrogramme.\n",
        "     - Ajoutez des couches `Conv2D` supplémentaires et des couches `MaxPooling2D` pour capturer des caractéristiques de niveau supérieur.\n",
        "   - **Couches Flatten et Dense** :\n",
        "     - Aplatissez la sortie finale et faites-la passer à travers une ou deux couches `Dense` pour la classification.\n",
        "     - Utilisez une activation softmax dans la couche `Dense` finale avec six unités de sortie, une pour chaque classe d'émotion.\n",
        "   - **Compiler et Entraîner** :\n",
        "     - Utilisez l'entropie croisée catégorielle comme fonction de perte et un optimiseur comme Adam.\n",
        "     - Entraînez le modèle sur l'ensemble d'entraînement, en utilisant un ensemble de validation pour ajuster les performances.\n",
        "\n",
        "#### 3. **Évaluation du Modèle**\n",
        "   - **Précision** :\n",
        "     - Évaluez la précision du modèle sur l'ensemble de test.\n",
        "   - **Matrice de Confusion** :\n",
        "     - Générez une matrice de confusion pour analyser quelles émotions sont bien classées et lesquelles sont fréquemment mal classées."
      ],
      "metadata": {
        "id": "c1q5D0iL6Lp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/CheyneyComputerScience/CREMA-D.git"
      ],
      "metadata": {
        "id": "Uut_uMSn62c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define path to CREMA-D dataset (update path accordingly)\n",
        "DATA_PATH = '/content/CREMA-D/AudioWAV'\n",
        "LABELS = {'ANG': 0, 'DIS': 1, 'FEA': 2, 'HAP': 3, 'NEU': 4, 'SAD': 5}"
      ],
      "metadata": {
        "id": "FKqAghja6hWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_audio(file_path, max_length=2.5, sr=16000):\n",
        "    audio, _ = librosa.load(file_path, sr=sr)\n",
        "    # Calculate the desired length in samples\n",
        "    target_length = int(sr * max_length)\n",
        "    # Adjust the audio to the desired length\n",
        "    audio = librosa.util.fix_length(audio, size=target_length)\n",
        "    # Convert audio to a mel-spectrogram\n",
        "    spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
        "    spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n",
        "    return spectrogram_db\n"
      ],
      "metadata": {
        "id": "a09BTeKV6iyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "def load_data(data_path, labels):\n",
        "    data, targets = [], []\n",
        "    for file_name in os.listdir(data_path):\n",
        "        if file_name.endswith('.wav'):\n",
        "            file_path = os.path.join(data_path, file_name)\n",
        "            label_str = file_name.split('_')[2]  # e.g., \"ANG\", \"DIS\"\n",
        "            if label_str in labels:\n",
        "                spectrogram = preprocess_audio(file_path)\n",
        "                data.append(spectrogram)\n",
        "                targets.append(labels[label_str])\n",
        "    return np.array(data), np.array(targets)"
      ],
      "metadata": {
        "id": "aT28mxSh6rQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "X, y = load_data(DATA_PATH, LABELS)\n",
        "X = X[..., np.newaxis]  # Add channel dimension for Conv2D\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "SOuPbGmW6x8j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
